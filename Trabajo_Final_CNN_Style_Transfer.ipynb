{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trabajo final: Diplomatura en Deep Learning - Año 2020\n",
    "## Alumno: Germán Dima"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qCY6UbkkI9_N"
   },
   "source": [
    "# Style Transfer\n",
    "\n",
    "<img src=\"https://i0.wp.com/chelseatroy.com/wp-content/uploads/2018/12/neural_style_transfer.png?resize=768%2C311&ssl=1\">\n",
    "\n",
    "La idea de este trabajo final es reproducir el siguiente paper:\n",
    "\n",
    "https://arxiv.org/pdf/1508.06576.pdf\n",
    "\n",
    "El objetivo es transferir el estilo de una imagen dada a otra imagen distinta. \n",
    "\n",
    "Como hemos visto en clase, las primeras capas de una red convolucional se activan ante la presencia de ciertos patrones vinculados a detalles muy pequeños.\n",
    "\n",
    "A medida que avanzamos en las distintas capas de una red neuronal convolucional, los filtros se van activando a medida que detectan patrones de formas cada vez mas complejos.\n",
    "\n",
    "Lo que propone este paper es asignarle a la activación de las primeras capas de una red neuronal convolucional (por ejemplo VGG19) la definición del estilo y a la activación de las últimas capas de la red neuronal convolucional, la definición del contenido.\n",
    "\n",
    "La idea de este paper es, a partir de dos imágenes (una que aporte el estilo y otra que aporte el contenido) analizar cómo es la activación de las primeras capas para la imagen que aporta el estilo y cómo es la activación de las últimas capas de la red convolucional para la imagen que aporta el contenido. A partir de esto se intentará sintetizar una imagen que active los filtros de las primeras capas que se activaron con la imagen que aporta el estilo y los filtros de las últimas capas que se activaron con la imagen que aporta el contenido.\n",
    "\n",
    "A este procedimiento se lo denomina neural style transfer.\n",
    "\n",
    "# En este trabajo se deberá leer el paper mencionado y en base a ello, entender la implementación que se muestra a continuación y contestar preguntas sobre la misma.\n",
    "\n",
    "# Una metodología posible es hacer una lectura rápida del paper (aunque esto signifique no entender algunos detalles del mismo) y luego ir analizando el código y respondiendo las preguntas. A medida que se planteen las preguntas, volviendo a leer secciones específicas del paper terminará de entender los detalles que pudieran haber quedado pendientes.\n",
    "\n",
    "Lo primero que haremos es cargar dos imágenes, una que aporte el estilo y otra que aporte el contenido. A tal fin utilizaremos imágenes disponibles en la web."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 437
    },
    "id": "kyHsa2t0SxZi",
    "outputId": "e72fcf52-62ed-42f1-f64e-cdb05d049797"
   },
   "outputs": [],
   "source": [
    "# Imagen para estilo\n",
    "!wget https://upload.wikimedia.org/wikipedia/commons/5/52/La_noche_estrellada1.jpg\n",
    "\n",
    "# Imagen para contenido\n",
    "!wget https://upload.wikimedia.org/wikipedia/commons/thumb/f/f4/Neckarfront_T%C3%BCbingen_Mai_2017.jpg/775px-Neckarfront_T%C3%BCbingen_Mai_2017.jpg\n",
    "\n",
    "# Creamos el directorio para los archivos de salida\n",
    "!mkdir /content/output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %tensorflow_version 1.x   # en Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 83
    },
    "id": "NIxH20o2eFoc",
    "outputId": "4785bcbb-4070-4e68-c2b5-4a1dfdccbad2"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import load_img, save_img, img_to_array\n",
    "import numpy as np\n",
    "from scipy.optimize import fmin_l_bfgs_b\n",
    "import time\n",
    "import argparse\n",
    "\n",
    "from keras.applications import vgg19\n",
    "from keras import backend as K\n",
    "from pathlib import Path\n",
    "\n",
    "# por si tira \"tf.gradients is not supported when eager execution is enabled. Use tf.GradientTape instead\"\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iLkV1bnFl_tK"
   },
   "outputs": [],
   "source": [
    "# Definimos las imagenes que vamos a utilizar, y el directorio de salida\n",
    "directorio = \"D:/Cosas Aca/Tp Final\"\n",
    "\n",
    "base_image_path = Path(directorio + \"/content/775px-Neckarfront_Tübingen_Mai_2017.jpg\")\n",
    "style_reference_image_path = Path(directorio + \"/content/La_noche_estrellada1.jpg\")\n",
    "result_prefix = Path(directorio + \"/content/output\")\n",
    "iterations = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gz2PeGfpeYzj"
   },
   "source": [
    "# 1) En base a lo visto en el paper ¿Qué significan los parámetros definidos en la siguiente celda?\n",
    "\n",
    "Respuesta:\n",
    "\n",
    "*total_variation_weight* = Peso de la *Loss* de suavizado, a la *Loss* global. Regula cuánta importancia se le da a que la imagen final no presente regiones ruidosas de definición (principalmente en los bordes).\n",
    "\n",
    "*style_weight* = Peso de la *Loss* de *Style*, a la *Loss* global (ec. 7 del artículo). Regula cuánta importancia tendrá el estilo de entrada sobre la imagen de salida.\n",
    "\n",
    "*content_weight* = Peso de la *Loss* de *Content*, a la *Loss* global (ec. 7 del artículo). Regula cuánta importancia tendrá el contenido de entrada sobre la imagen de salida.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P9Dt3aaEmJWS"
   },
   "outputs": [],
   "source": [
    "total_variation_weight = 0.1\n",
    "style_weight = 10\n",
    "content_weight = 1\n",
    "# Recomiendan en el artíuclo content_weight / style_weight ~ 10^-3 , 10^-4 (en el caso total_variation_weight  = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CQQJOhCVuse6"
   },
   "outputs": [],
   "source": [
    "# Definimos el tamaño de las imágenes a utilizar\n",
    "width, height = load_img(base_image_path).size\n",
    "img_nrows = 400\n",
    "img_ncols = int(width * img_nrows / height)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gg2ct-8agm1E"
   },
   "source": [
    "# 2) Explicar qué hace la siguiente celda. En especial las últimas dos líneas de la función antes del return. ¿Por qué?\n",
    "\n",
    "Ayuda: https://keras.io/applications/\n",
    "\n",
    "Respuesta:\n",
    "\n",
    "La siguiente función (1) carga una imagen y la re-escalea a dimensión: (img_nrows x img_ncols), (2) la convierte en un array numérico de dimensión: (img_nrows,img_ncols,3 -que es el número de canales-), (3) para el manejo en batch, agrega una dimensión extra en la primera componente: (1,img_nrows,img_ncols,3), (4) para un correcto funcionamiento de la VGG19, el array es convertido de RGB a BGR, para luego centrar cada canal (sin normalizar) respecto del dataset ImageNet (con el que fue entrenado la red)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tAkljg4zuzYd"
   },
   "outputs": [],
   "source": [
    "def preprocess_image(image_path):\n",
    "    img = load_img(image_path, target_size=(img_nrows, img_ncols))\n",
    "    img = img_to_array(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = vgg19.preprocess_input(img)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KTf0YDSagt10"
   },
   "source": [
    "# 3) Habiendo comprendido lo que hace la celda anterior, explique de manera muy concisa qué hace la siguiente celda. ¿Qué relación tiene con la celda anterior?\n",
    "\n",
    "Respuesta:\n",
    "\n",
    "La siguiente función puede pensarse como la inversa de la función \"preprocess_image\", ya que (1) convierte una imagen al formato (alto,ancho,canales) -ignorando la componente del batch size-, (2) descentra cada canal respecto de los valores medios del dataset ImageNet (103.939, 116.779, 123.68) y (3) vuelve a reordenar para pasar del formato BGR a RGB. El comando \"clip\", en el rango 0 a 255, garantiza que sea un color válido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y5LaTrsAu14z"
   },
   "outputs": [],
   "source": [
    "def deprocess_image(x):\n",
    "    x = x.reshape((img_nrows, img_ncols, 3))\n",
    "    # Remove zero-center by mean pixel\n",
    "    x[:, :, 0] += 103.939\n",
    "    x[:, :, 1] += 116.779\n",
    "    x[:, :, 2] += 123.68\n",
    "    # 'BGR'->'RGB'\n",
    "    x = x[:, :, ::-1]\n",
    "    x = np.clip(x, 0, 255).astype('uint8')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HYNio09mu4S3"
   },
   "outputs": [],
   "source": [
    "# get tensor representations of our images\n",
    "# K.variable convierte un numpy array en un tensor, para \n",
    "base_image = K.variable(preprocess_image(base_image_path))\n",
    "style_reference_image = K.variable(preprocess_image(style_reference_image_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "id": "a1Lbw02Uu--o",
    "outputId": "6cc926fa-55af-43fa-fe91-3b68c0910502"
   },
   "outputs": [],
   "source": [
    "combination_image = K.placeholder((1, img_nrows, img_ncols, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RJEi0YI3Uzrm"
   },
   "source": [
    "Aclaración:\n",
    "\n",
    "La siguiente celda sirve para procesar las tres imagenes (contenido, estilo y salida) en un solo batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gGO_jGFfvEbF"
   },
   "outputs": [],
   "source": [
    "# combine the 3 images into a single Keras tensor\n",
    "input_tensor = K.concatenate([base_image,\n",
    "                              style_reference_image,\n",
    "                              combination_image], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "tdG59VRavHGB",
    "outputId": "a133befb-68d1-4c51-99e6-417c1103f726"
   },
   "outputs": [],
   "source": [
    "# build the VGG19 network with our 3 images as input\n",
    "# the model will be loaded with pre-trained ImageNet weights\n",
    "model = vgg19.VGG19(input_tensor=input_tensor,\n",
    "                    weights='imagenet', include_top=False)\n",
    "print('Model loaded.')\n",
    "\n",
    "# get the symbolic outputs of each \"key\" layer (we gave them unique names).\n",
    "outputs_dict = dict([(layer.name, layer.output) for layer in model.layers])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "70-vs_jZkKVc"
   },
   "source": [
    "# 4) En la siguientes celdas:\n",
    "\n",
    "- ¿Qué es la matriz de Gram?¿Para qué se usa?\n",
    "- ¿Por qué se permutan las dimensiones de x?\n",
    "\n",
    "La matriz de Gram almacena la información de las correlaciones entre las distintas respuestas de los filtros. Al ser generada como el producto interno entre filtros (aplanados), esta matriz es usada para captar la esencia de estilos (altos valores en sus elementos implicarían que los filtros que lo componen se han activado en esa posición espacial). Matemáticamente se busca minimizar la distancia cuadrática media entre la matriz de Gram de la imagen orginal (de estilo) y la generada por el modelo.\n",
    "\n",
    "Según su definición en el artículo (ec. 3), el elemento (i,j) de la matriz se computa como el producto escalar sobre los filtros $F_{i,j}$ ahora pensados como vectores. Esto nos devuelve una matriz cuadrada cuya dimensiones corresponden al producto de las dimensiones de alto y ancho de los mapas de _features_.\n",
    "\n",
    "Yendo a la definición de la siguiente celda, si \"x\" es de dimensión (25,32,512) -alto,ancho,cantidad de filtros en esa capa-, el comando \"permute\" lo transformará en dimensión (512,25,32) para luego pasara a dimensión (512,800) mediante el \"flatten\" (aplana \"alto\" y \"ancho\"). Por lo tanto:\n",
    "$\\overline{features}\\in {{\\mathbb{R}}^{\\text{512}\\times \\text{800}}}* {{\\overline{features}}^{T}}\\in {{\\mathbb{R}}^{\\text{800}\\times \\text{512}}}=gram\\in {{\\mathbb{R}}^{\\text{512}\\times 512}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K1FODPATvJ1k"
   },
   "outputs": [],
   "source": [
    "def gram_matrix(x):\n",
    "    features = K.batch_flatten(K.permute_dimensions(x, (2, 0, 1)))\n",
    "    gram = K.dot(features, K.transpose(features))\n",
    "    return gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vBQkKFY0Rbx-"
   },
   "source": [
    "# 5) Losses:\n",
    "\n",
    "Explicar qué mide cada una de las losses en las siguientes tres celdas.\n",
    "\n",
    "Rta:\n",
    "\n",
    "La función \"style_loss\" da una noción de similitud de estilos entre la imagen generada y la original (de estilo). Computa la contribución de una capa en particular a la *Loss* general (ec. 4) mediante la distancia cuadrática media entre las matrices de Gram (de la imagen original y la generada).\n",
    "\n",
    "La función \"content_loss\" responde al contenido entre la imagen generada frente a la original (de contenido). Se calcula mediante el error cuadrático (ec. 1) entre el array de una capa y la imagen original.\n",
    "\n",
    "La función \"total_variation_loss\" es un suavizado espacial, actuándo como regularizador de la imagen. Computa la suma de las diferencias absolutas de los valores de píxeles vecinos. Su minimización conlleva a un filtro de picos abruptos de intensidad de píxeles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1-Gt0ahWvN6q"
   },
   "outputs": [],
   "source": [
    "def style_loss(style, combination):\n",
    "    assert K.ndim(style) == 3\n",
    "    assert K.ndim(combination) == 3\n",
    "    S = gram_matrix(style)\n",
    "    C = gram_matrix(combination)\n",
    "    channels = 3\n",
    "    size = img_nrows * img_ncols\n",
    "    return K.sum(K.square(S - C)) / (4.0 * (channels ** 2) * (size ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XCqnju5RvQCo"
   },
   "outputs": [],
   "source": [
    "def content_loss(base, combination):\n",
    "    return K.sum(K.square(combination - base))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "udEp5h31vRnY"
   },
   "outputs": [],
   "source": [
    "def total_variation_loss(x):\n",
    "    assert K.ndim(x) == 4\n",
    "    a = K.square(\n",
    "        x[:, :img_nrows - 1, :img_ncols - 1, :] - x[:, 1:, :img_ncols - 1, :])\n",
    "    b = K.square(\n",
    "        x[:, :img_nrows - 1, :img_ncols - 1, :] - x[:, :img_nrows - 1, 1:, :])\n",
    "    return K.sum(K.pow(a + b, 1.25))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-65vcinbvTZ0"
   },
   "outputs": [],
   "source": [
    "# Armamos la loss total\n",
    "loss = K.variable(0.0)\n",
    "layer_features = outputs_dict['block5_conv2']\n",
    "base_image_features = layer_features[0, :, :, :]\n",
    "combination_features = layer_features[2, :, :, :]\n",
    "loss = loss + content_weight * content_loss(base_image_features,\n",
    "                                            combination_features)\n",
    "\n",
    "feature_layers = ['block1_conv1', 'block2_conv1',\n",
    "                  'block3_conv1', 'block4_conv1',\n",
    "                  'block5_conv1']\n",
    "for layer_name in feature_layers:\n",
    "    layer_features = outputs_dict[layer_name]\n",
    "    style_reference_features = layer_features[1, :, :, :] \n",
    "    combination_features = layer_features[2, :, :, :]\n",
    "    sl = style_loss(style_reference_features, combination_features)\n",
    "    loss = loss + (style_weight / len(feature_layers)) * sl\n",
    "loss = loss + total_variation_weight * total_variation_loss(combination_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 92
    },
    "id": "pbz4n1OhvV2K",
    "outputId": "c2b208c6-7ddd-4a40-eeda-525f0809b963"
   },
   "outputs": [],
   "source": [
    "grads = K.gradients(loss, combination_image)\n",
    "\n",
    "outputs = [loss]\n",
    "if isinstance(grads, (list, tuple)):\n",
    "    outputs += grads\n",
    "else:\n",
    "    outputs.append(grads)\n",
    "\n",
    "f_outputs = K.function([combination_image], outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1JbydbOaVcvU"
   },
   "source": [
    "# 6) Explique el propósito de las siguientes tres celdas. ¿Qué hace la función fmin_l_bfgs_b? ¿En qué se diferencia con la implementación del paper? ¿Se puede utilizar alguna alternativa?\n",
    "\n",
    "Respuesta:\n",
    "\n",
    "En el artículo original los autores utilizan el método de BFGS para la minimización de la *Loss*. Este algoritmo determina en qué dirección desplazarse y la distancia a avanzar, mediante el cálculo de la Hessiano. El método requiere que se pase el valor de la *Loss* y el valor del gradiente como funciones por separado. Adicionalmente, el optimizador opera sobre vectores planos (en vez de arrays de dimensión tres). La función de la celda siguiente apunta en resolver estos dos asuntos. \n",
    "\n",
    "En este código se utiliza una variante del optimizador: L-BFGS-B (que viene con SciPy, y se llama mediante \"fmin_l_bfgs_b\") el cual permite agregar condiciones de borde constantes a los parámetros (${{\\min }_{i}}\\le {{\\theta }_{i}}\\le {{\\max }_{i}}$) además de un uso más restringido de memoria.\n",
    "\n",
    "Como dice el comentario de la celda, no sería eficiente calcular el valor de la *Loss* y el valor de los gradientes de forma independiente. Para evitar esto, se crea una clase de llamada *Evaluator* la cual calcula estos valores a la vez, devolviendo el valor de las *Loss* cuando se la llame la primera vez y almacenará en caché los gradientes para las siguientes llamadas.\n",
    "\n",
    "El post del blog https://blog.slavv.com/picking-an-optimizer-for-style-transfer-86e7b8cba84b muestra una serie de experimentos en donde prueban diferentes alternativas de optimizadores para este tipo de procesamiento de imágenes. Si bien Adagrad, Adam y L-BFGS muestran una performance superior, este último presenta una significativa mejoría. Una de las razones podría atribuirse a que no existe ningún proceso estocástico en el cálculo: el optimizador siempre recibe la misma imagen (determinística)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zVE1_qemvZeN"
   },
   "outputs": [],
   "source": [
    "def eval_loss_and_grads(x):\n",
    "    x = x.reshape((1, img_nrows, img_ncols, 3))\n",
    "    outs = f_outputs([x])\n",
    "    loss_value = outs[0]\n",
    "    if len(outs[1:]) == 1:\n",
    "        grad_values = outs[1].flatten().astype('float64')\n",
    "    else:\n",
    "        grad_values = np.array(outs[1:]).flatten().astype('float64')\n",
    "    return loss_value, grad_values\n",
    "\n",
    "# this Evaluator class makes it possible\n",
    "# to compute loss and gradients in one pass\n",
    "# while retrieving them via two separate functions,\n",
    "# \"loss\" and \"grads\". This is done because scipy.optimize\n",
    "# requires separate functions for loss and gradients,\n",
    "# but computing them separately would be inefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qbl9roIgvdb1"
   },
   "outputs": [],
   "source": [
    "class Evaluator(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.loss_value = None\n",
    "        self.grads_values = None\n",
    "\n",
    "    def loss(self, x):\n",
    "        assert self.loss_value is None\n",
    "        loss_value, grad_values = eval_loss_and_grads(x)\n",
    "        self.loss_value = loss_value\n",
    "        self.grad_values = grad_values\n",
    "        return self.loss_value\n",
    "\n",
    "    def grads(self, x):\n",
    "        assert self.loss_value is not None\n",
    "        grad_values = np.copy(self.grad_values)\n",
    "        self.loss_value = None\n",
    "        self.grad_values = None\n",
    "        return grad_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sb0yOEl-WOE6"
   },
   "source": [
    "# 7) Ejecute la siguiente celda y observe las imágenes de salida en cada iteración."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "n31YBwCVvhAI",
    "outputId": "4c1bf03c-9d66-48ea-93f2-4489fc20beaa"
   },
   "outputs": [],
   "source": [
    "9hb 8i0evaluator = Evaluator()\n",
    "\n",
    "# run scipy-based optimization (L-BFGS) over the pixels of the generated image\n",
    "# so as to minimize the neural style loss\n",
    "x = preprocess_image(base_image_path)\n",
    "\n",
    "for i in range(/u83):\n",
    "    print('Start of iteration', i)\n",
    "    start_time = time.time()\n",
    "    x, min_val, info = fmin_l_bfgs_b(evaluator.loss, x.flatten(),\n",
    "                                     fprime=evaluator.grads, maxfun=20)\n",
    "    print('Current loss value:', min_val)\n",
    "    # save current generated image\n",
    "    img = deprocess_image(x.copy())\n",
    "    fname = result_prefix / ('output_at_iteration_%d.png' % i)\n",
    "    save_img(fname, img)\n",
    "    end_time = time.time()\n",
    "    print('Image saved as', fname)\n",
    "    print('Iteration %d completed in %ds' % (i, end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para colab\n",
    "if(False):\n",
    "    from google.colab import drive, files\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    # Zippeo las imágenes y las descargo    \n",
    "    !zip -r \"/content/test\" \"/content/output\"\n",
    "    files.download(\"test.zip\") \n",
    "    \n",
    "    # Nombre\n",
    "    file_name = file_name = str(\"generadas_\" + str(total_variation_weight) + \"_\" + str(style_weight) + \"_\" + str(content_weight) + \".zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SkiJtofbWWy1"
   },
   "source": [
    "# 8) Generar imágenes para distintas combinaciones de pesos de las losses. Explicar las diferencias. (Adjuntar las imágenes generadas como archivos separados.)\n",
    "\n",
    "Respuesta:\n",
    "\n",
    "Para poder estudiar la importancia de los pesos, se optó por utilizar una imagen de referencia (generada con parámetros ${{\\left( \\text{total }\\!\\!\\_\\!\\!\\text{ variation},\\text{style},\\text{content} \\right)}_{\\text{weight}}}\\text{ = }\\left( \\text{0}\\text{.1}\\text{,10}\\text{,1} \\right)$ y compaginar todas las imágenes en una sola para una mejor comparación de las mismas. Adicionalmente, para seguir lo pedido en el enunciado, se han adjuntando las imágenes por separado.\n",
    "\n",
    "Al aumentar el valor del peso de la *Loss* de suavizado, se observa un aumento de importancia a la continuidad de trazos de aquella *Loss* dominante. En la primera fila de la siguiente figura se han generado imágenes cuyo peso de la *Loss* de *syle* es diez veces mayor al del *content*, para luego ir incrementando el valor del peso del suavizado, sin modificar los restantes hiperparámetros. Se aprecia que estos cambios enfatizan los colores, previniendo saltos de tonalidades dados por la contribución del estilo. Por el contrario, si la relación entre estilo y contenido se invierte (segunda fila), el suavizado ataca a la rugosidad de la imagen (ver los techos de las casas). Cuando esta *Loss* adquiere un peso significativamente mayor a las anteriores, esta homogeneización provoca un efecto de desenfoque. Por otro lado, no se han encontrado diferencias a ojo desnudo para valores de esta *Loss* iguales a 0.005 y 0.\n",
    "\n",
    "<img src=\"./figuras/test_suavidad.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_img(\"./figuras/test_suavidad.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_img(\"./figuras/generadas_0.1_10_1.png\")   # total_variation = 0.1 / style = 10 / content = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_img(\"./figuras/generadas_10_10_1.png\")    # total_variation = 10 / style = 10 / content = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_img(\"./figuras/generadas_100_10_1.png\")    # total_variation = 100 / style = 10 / content = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_img(\"./figuras/generadas_0.1_1_10.png\")    # total_variation = 0.1 / style = 1 / content = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_img(\"./figuras/generadas_10_1_10.png\")    # total_variation = 10 / style = 1 / content = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_img(\"./figuras/generadas_100_1_10.png\")    # total_variation = 100 / style = 1 / content = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como fue mencionado anteriormente, los hiperparámetros *style_weight* y *content_weight* actúan como reguladores del estilo y contornos en la imagen generada. Para una mejor apreciación de sus roles se han generado imágenes con valores extremos. En la primera fila de la siguiente figura se aprecia como, al aumentar el hiperparámetro *style_weight*, no sólo los colores empiezan a virar hacia la imagen de referencia, sino también ciertas regiones empiezan a deformarse (ver el cielo a la altura de los tejados y la pared en contacto con el agua). El mismo fenómeno puede apreciarse al exagerarse el hiperparámetro *content_weight* (segunda fila), en donde el cielo y las casas tienden a parecerse al de la figura de referencia, quedando apenas leves irregularidades, del aporte del cuadro.\n",
    "\n",
    "<img src=\"./figuras/test_estilo_bordes.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_img(\"./figuras/test_estilo_bordes.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_img(\"./figuras/generadas_0.1_1_1.png\")  # total_variation = 0.1 / style = 1 / content = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_img(\"./figuras/generadas_0.1_10_1.png\")    # total_variation = 0.1 / style = 10 / content = 1 (referencia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_img(\"./figuras/generadas_0.1_1000_1.png\")    # total_variation = 0.1 / style = 1000 / content = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_img(\"./figuras/generadas_0.1_10_0.1.png\")  # total_variation = 0.1 / style = 10 / content = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_img(\"./figuras/generadas_0.1_10_1000.png\")    # total_variation = 0.1 / style = 10 / content = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9) Cambiar las imágenes de contenido y estilo por unas elegidas por usted. Adjuntar el resultado.\n",
    "\n",
    "Respuesta:\n",
    "\n",
    "Del mismo modo que en la respuesta anterior, se adjunta una imagen global (con las imágenes originales de bordes y estilos, y la generada por el modelo) y luego cada imagen por separado. En los comentarios se encuentran los pesos utilizados en el formato ${{\\left( \\text{total }\\!\\!\\_\\!\\!\\text{ variation},\\text{style},\\text{content} \\right)}_{\\text{weight}}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img src=\"./figuras/personal_01.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_img(\"./figuras/personal_01.png\")   # Pesos de las Loss: (0.5 , 3 , 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_img(\"./figuras/personal_01_content.jpg\")   # Imagen de \"content\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_img(\"./figuras/personal_01_style.jpg\")   # Imagen de \"syle\" (rotada para mejores resultados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_img(\"./figuras/personal_01_final.png\")   # Imagen generada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img src=\"./figuras/personal_02.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_img(\"./figuras/personal_02.png\")   # Pesos de las Loss: (0.1 , 3 , 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_img(\"./figuras/personal_02_content.jpg\")   # Imagen de \"content\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_img(\"./figuras/personal_02_style.jpg\")   # Imagen de \"syle\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_img(\"./figuras/personal_02_final.png\")   # Imagen generada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  <img src=\"./figuras/personal_03.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_img(\"./figuras/personal_03.png\")   # Pesos de las Loss: (0.1 , 3 , 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_img(\"./figuras/personal_03_content.jpg\")   # Imagen de \"content\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_img(\"./figuras/personal_03_style.jpg\")   # Imagen de \"syle\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_img(\"./figuras/personal_03_final.png\")   # Imagen generada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img src=\"./figuras/personal_04.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_img(\"./figuras/personal_04.png\") \n",
    "# Pesos de las Loss: (0.5 , 5 , 1), aumento en la loss de suavizado para resaltar más el fondo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_img(\"./figuras/personal_04_content.jpg\")   # Imagen de \"content\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_img(\"./figuras/personal_04_style.jpg\")   # Imagen de \"syle\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_img(\"./figuras/personal_04_final.png\")   # Imagen generada"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Trabajo Final CNN - Style Transfer.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
